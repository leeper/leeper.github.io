---
layout: post
title: Challenges and Opportunities in Scientific Peer Review
tags:
 - open science
 - peer review
 - publishing
 - science
 - journals
 - academia
---

What is the point of peer review? In light of [a seemingly endless stream of scientific retractions](http://retractionwatch.com/), reproducibility failures like [the Reinhart-Rogoff data glitch](http://en.wikipedia.org/wiki/Growth_in_a_Time_of_Debt), [p-hacking](http://datacolada.org/), [the "Replication Crisis" in psychology](http://chronicle.com/article/Replication-Crisis-in/147301/), ["crappy, or just plain pointless" peer-reviewed research](http://andrewgelman.com/2014/03/06/much-time-spend-criticizing-research-thats-fraudulent-crappy-just-plain-pointless/), a suspicion that ["Most Published Research Findings Are False"](http://www.plosmedicine.org/article/info%3Adoi%2F10.1371%2Fjournal.pmed.0020124), and numerous cases of [misconduct](http://en.wikipedia.org/wiki/Andrew_Wakefield) or [outright](http://www.nytimes.com/2013/04/28/magazine/diederik-stapels-audacious-academic-fraud.html?pagewanted=all) [scientific](http://www.the-scientist.com/?articles.view/articleNo/26647/title/Grad-student-falsified-data/) [fraud](http://en.wikipedia.org/wiki/Marc_Hauser), the role of peer review in contemporary science is in the cross-hairs. If the diffuse enterprise of science frequently (and perhaps increasingly) produces flawed knowledge (either intentionally or unintentionally), what value does the peer review process have for science today and in the future? 


## Seven Issues with Peer Review ##

Peer review may be vital to science: it is what allows scientists and the public as a whole to distinguish the collective body of substantiated scientific evidence from the noise of opinion, speculation, and lies. But the process of peer review is broken. How so? Below are seven problems with scientific peer review (especially in the social sciences), which I address in detail in the remainder of this post. In the end, I build on these challenges to suggest directions science should move to improve peer review and the dissemination of scientific claims.

 1. We implicitly rely on peer review to distinguish what is true from what is false, but this is not something peer review can reasonably be expected to achieve. In many disciplines, an enormous body of information is published in non-peer-reviewed outlets (e.g., in book form) and treated as scientifically equivalent to information published in peer-reviewed scientific journals. 
 
 2. We use peer review to make decisions about what is "publishable" and what is not, which leads to troubling biases in published research literatures.
 
 3. Though this is changing in some disciplines, scientific research can be judged sound and subsequently published without being reproducible. That is, the data analytic, computational, and interpretive steps that link raw scientific materials (observational units) are intransparent despite those steps being the essential elements of a credible scientific claim.

 4. Peer review is conducted (in almost all disciplines) in private, shadowed in secrecy, and exclusively prior to publication, which treats publication as a definitive, final scientific statement that cannot be revised except through retraction or an antiquated method of subsequently published errata.
 
 5. Peer review operates under the oversight of journal editors who have essentially unrestricted editorial control, which means that double-blind peer review exists in name-only. Editorial control may actually have significant advantages for the quality of scientific publishing, but it introduces a tension between the lauded notion of anonymous peer review (arguably meant to judge scientific contributions in an uniased fashion) and the objective of journal metrics (e.g., citations/impact factor, reputation, or revenue).
 
 6. Peer review is feasible because of the (generally) uncompensated labor of scientists, in either monetary or employment terms, for the financial benefit of academic publishers.
 
 7. The lack of peer review is often associated with scam publishers. This creates a detrimental association between scientific goods (e.g., open access to research, fast review times) and questionable practices (e.g., lack of peer review, pay for publication, fraud).
 
 
### 1. Peer Review as Search for Truth ###

Science is knowledge (the word science being derived from the Latin *scientia* meaning knowledge), both in process and in content. Scientific knowledge is understanding about the world derived from a systematic testing of propositions about the world. Peer review is often seen as the arbiter of what is "true" and what is "false". If something is "not yet peer reviewed," there is often hesitation to count particular research findings in the collective body of scientific knowledge.

This is with good reason: some claims are not substantiated and scientific peer review can reveal fatal flaws that means particular claims should never enter the corpus of human knowledge. Yet we also know that there is a considerable body of claims that have passed peer review but that do not seem to meet the collective standards of scientific knowledge. Scientific retractions due to misanalyzed data and other errors, or due simply to fraud, are a prevalent part of contemporary science (as the examples that opened this post make obvious). 

Additionally important, and often overlooked in conversations about peer review in science, is that there is a considerable body of claims that scientists readily accept as credible knowledge claims despite lacking peer review. Among these are working papers (which are the bread-and-butter of contemporary economics research), conference papers (many of which are never subjected to peer review or even published), book chapters in edited volumes (that are edited but rarely peer reviewed), and finally books themselves (which, while again edited by someone, are rarely if ever subject to the kind of rigorous peer review that articles receive at scientific journals). In political science, we accept monographs as equally (if not more important) knowledge claims than peer-reviewed research articles. Publishing a book at a "top" university press is often (nearly) sufficient grounds for tenure, awards are handed out at every scientific conference for the "best book" in particular fields, and books receive numerous citations. All this despite books not receiving a transparent process of scientific peer review.

So, despite (social) scientists lauding peer review as what distinguishes credible from not credible knowledge claims, our collective body of claims is populated by a mix of both peer-reviewed and un-reviewed claims that are treated more or less equally. Peer review therefore does not, in practice, distinguish what is thought to be true from what is thought to be false.

More importantly, however, is the supposition in peer review that truth is itself knowable at all. Truth is an unobtainable ideal. Following from Plato, philosophers frequently define *knowledge* as [justified, true belief](http://en.wikipedia.org/wiki/Belief#Justified_true_belief): beliefs about the world that are justified by evidence and that are true in some absolute sense. We cannot in many cases - particularly in the social sciences - ever observe the "truth", so all we have are justified beliefs that may or may not be true. Peer review is a process of evaluating whether the justifications for beliefs are sufficiently credible to enter the scientific corpus. It is not a process of evaluating whether beliefs are true, because doing so is definitionally impossible. 

I frequently read reviewer reports that question the truth of particular research contributions. Such reports reflect a fundamental misunderstanding: peer review does not arbitrate between what is true and what is false, but between what is well-justified and what is not well-justified. Reviewers cannot be asked to and should not attempt to evaluate whether something is true or false, only whether the beliefs inferred from the evidence are justified by the methods of inquiry employed.

Whether the beliefs implied by a study are in some sense "correct" should not be the basis for publication decisions - lest it contribute to publication biases - and is instead something that can only be evaluated by the repeated investigation of a phenomenon. We can never know if our knowledge claims are *true* but we can evaluate whether they are consistent, but that is a job for the meta-analyst not for the peer reviewer.


### 2. Publication Bias ###

Peer review, as described so far, is a process of evaluating whether knowledge claims are justified by methods employed. It should therefore be obvious that the content of findings (i.e., the beliefs implied by a study, or the knowledge claims expressed) cannot be peer reviewed because such review is merely an expression of whether the knowledge claims comply with the prior beliefs held by the peer reviewer. Whether the claims and those prior beliefs align is not informative about whether the knowledge claims are justified by the underlying scientific method.

Despite the fact that this should be obvious - peer review evaluates science, not the findings thereof - peer reviewing in my experience is often about the knowledge claims themselves, not the methods by which those claims are derived. For example, I've recently received criticism on a paper during the peer review process that the results from multiple tests were not sufficiently consistent with the theory being advanced. Stated another way, the reviewer was saying that the research would have been had the results been different than they actually were - the review was about the results, not the process of arriving at those results. 

Similarly, I've frequently seen reviewers (on my papers and those that I have reviewed) pointing out a particular statistic of interest was not statistically distinguishable from zero. This kind of attention to the content of knowledge claims (without regard for the quality of the methods underlying those claims) is now widely seen as a major source of publication bias. The use of statistical significance thresholds as the basis for what is "good" and what is "bad" science (and what are informative or uninformative claims) is therefore fundamentally at odds with the justification-evaluating role of peer review. Yet, this approach to outcome-focused review persists.

A critic might argue against the perspective advanced here by saying that, in the end, journals must make a distinction between what can and what cannot be published in a specific outlet and while scientific (read, methodological) quality and the importance of the topic may eliminate truly unpublishable papers, the distinction drawn between what is and what is not published must therefore be made among the remaining papers on the basis of results alone. Results deemed "interesting," "important," or "broadly relevant" therefore make their way into publication at top outlets, while results that are of equivalent scientific quality but have different patterns of findings are not published in these outlets. The *Journal of Personality and Social Psychology*'s long-standing policy of not publishing replications (a policy also followed by the *American Political Science Review*) further mean that papers potentially published precisely because of their results (see [Daryl Bem's research on precognition](http://www.theguardian.com/science/2012/mar/15/precognition-studies-curse-failed-replications)) are then not subject to scientific critique that finds distinct results for research of exactly equivalent methodological quality.

A particularly troubling aspect of the role of peer review in creating and perpetuating peer review is that judgements about the scientific quality of the justifications underlying knowledge claims do not need to be tied to decisions about publication. When a paper has passed peer review, that is a badge indicating some minimal amount of quality and there is no fundamental limit on the number of research articles that can earn the badge of peer review. 

There is, however, a finite limit on the number of pages of research academic publishers are willing to print. This is an artificial scarcity in the digital age: there is no limit to the amount of research that can be "published". [Arxiv](http://arxiv.org/), [the NBER working paper series](https://ideas.repec.org/s/nbr/nberwo.html), and [PLoS](http://www.plos.org/) among others show that print publication is no longer a necessary constraint on the dissemination of scientific research. As such, peer review - which has no direct relationship to publication - has been co-opted by academic publishers. The "best" research is published and research that is anything but "best" is not published, at least not in top outlets. All of that research might eventually earn the badge of peer review, but the artificial scarcity of print publication creates value for publishers because these "top" outlets have convinced reviewers to engage in peer review for the purpose of judging publishability (often on the basis of results) rather than evaluating the justifiability of knowledge claims alone.



### 3. Transparency ###

The *American Journal of Political Science* has [just announced](http://ajps.org/2015/03/26/the-ajps-replication-policy-innovations-and-revisions/) a novel (to political science) policy of automatically reproducing all results included in quantitative articles that have been accepted for publication. This policy rightly steers the review process toward the transparency of methods and the reproducibility of knowledge claims. This policy builds on a growing push for ["data access and research transparency"](http://blog.oup.com/2014/11/replication-data-access-transparency-social-science/) in political science that has previously involved a number of journals requiring data publication and, better, the publication of complete reproduction code for all analyses.





### 4. Science is Replication ###

As soon as peer review is directly tied to publication, publications obtain an undeserved aura of truth. Peer review cannot reveal the truth of claims, only whether they are justified given methods and observations. Publication is nothing more than peer review and it is often much, much less than peer review. Simply because something is published does not make it true. Simply because something is peer reviewed does not make it true. A fundamental part of the collective scientific process is the progressive updating of collective beliefs, which necessarily involves *post*-publication evaluations of research methods, data, and findings.



discuss replication


discuss retraction


discuss post-publication review and the need for contributions that can be amended without being retracted





### 5. Editorial Control ###

 
 


### 6. Uncompensated Labor ###

Peer review is arguably vital to the scientific process.






### 7. Distinguishing Science from Scams ###



discuss how peer review has no clear meaning

problematizes good ideas (open science)







## What is the Purpose of Peer Review? ##

When we review, is it our job as scientists to review the theory, research design, the implementation of the research, the usefulness of the dataset, the particulars of the analysis, the presentation and discussion of the analysis, or the quality of writing in the paper? In theory, it is all of these things. Yet the reviewer is limited in their capacity by time, resources, access to research products, the post-hoc nature of reviewing, and so forth. In practice, reviewing is a largely editorial exercise: it involves the careful reading of a written article; critique of prose, structure, argumentation, and presentation; and (depending on the reviewer, author, journal, and manuscript) some amount of substantive scientific criticism of data collection, analysis, and other methods, but so many of those features are already executed and therefore not easily amended that peer review becomes a "pass"/"fail" judgement about eligibility for publication rather than a review leading to substantive revision and improvement of a scientific contribution.





Reveal truth

Information aggregation.


Uncover fraud

Detect scientific errors

Separate "good" from "bad" research

Honesty
 - Honest reporting of research design (cite protocol article
 - Honest reporting of mistakes (cite Lupia's lab notebook article)
 - Honest


Journals as recommendation engines in a world of artificial scarcity
 

There is disagreement about how peer review should work. Some, like the [The Winnower](https://thewinnower.com/) and [F1000Research](http://f1000research.com/), argue that post-publication peer review is a viable alternative to the current norm of pre-publication peer review.

Single-blind review, non-blind review (PLoS)

Fast review cycles

Cascading reviewing reports

Pre-registration, which originated in clinical studies where positive/significant results are systematically selected out of a stream of research studies.The problem with pre-registration is that it must come with publication pre-acceptance, but this requires registrars to be linked to publication venues and in most cases they are not.

Outcome blind review

Bifurcated peer review: First phase improves manuscript, second phase decides publication venue/profile

Reproducibility and especially data access/archiving 


Distinguish peer review of grants, peer review of published research, IRB review, review by way of subsequent scientific debate, Altmetrics, positive and negative citations, reproducible (e.g., internal reproduction of results by an editorial assistant), subsequent replication, etc.


History:
 - Biagioli
 - Bornmann, http://www.lutz-bornmann.de/icons/sciencereviews.pdf
 - Bornmann, http://www.ns2.okcir.com/Articles%20VI%202/LutzBornmann-FM.pdf
 - Kronick on journalism, http://www.ncbi.nlm.nih.gov/pubmed/2406469
 - Spier, http://www.cell.com/trends/biotechnology/abstract/S0167-7799(02)01985-6?_returnURL=http%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0167779902019856%3Fshowall%3Dtrue
 - Relman, http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1002603/pdf/westjmed00111-0058.pdf




[Open Journal Systems](https://pkp.sfu.ca/ojs/)

[Kieran Healy's "retired papers"](http://kieranhealy.org/publications/)

http://www.theguardian.com/higher-education-network/blog/2014/aug/05/why-we-should-publish-less-scientific-research

[incentive experiment](http://chrisblattman.com/2014/08/21/peer-review-experiment/)

[resubmitted articles rejected](http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=6577844&utm_content=buffer6caaa&utm_source=buffer&utm_medium=twitter&utm_campaign=Buffer)


Nature debate:
http://www.nature.com/nature/peerreview/debate/nature05032.html
http://www.nature.com/nature/peerreview/debate/nature04990.html


http://blogs.scientificamerican.com/guest-blog/2011/11/02/what-is-peer-review-for/

CDC Guidelines: http://www.cdc.gov/maso/pdf/PeerReview.pdf


Times HigherEd article: https://www.timeshighereducation.co.uk/content/the-peer-review-drugs-dont-work



## Moving Forward ##

If the current practice of peer review is so problematic, where can we go from here? There is clearly no widespread agreement about how the scientific review process should work and I don't attempt to have all the answers. I think there are a handful of simple actions that we can start taking now to improve the process of scientific peer review in the social sciences and elsewhere.

Things you as an individual scientist can do:

 1. Engage in outcome-blind reviewing. Do not judge the quality of papers based on the content of their findings.
 
 2. Publish in open venues to the extent possible. This remains a challenge in the social sciences, but [*Research and Politics*](http://www.uk.sagepub.com/researchandpolitics/#.VSplgpGqqko), [PLoS](http://www.plos.org/), and [the proposed APSA open access journal](http://www.apsanet.org/files/New%20Journal%20Proposal%20FINAL%20Web.pdf) are strating to change this for political science researchers.
 
 3. Re-imagine your role as reviewer to be one of advisor, counselor, and editor rather than critic. When reviewing research that has already been conducted, it's nearly impossible to improve the justifications of knowledge claims, so instead all peer reviewers can and should do is focus on the truthful reporting of results (which often means *not* asking for additional analyses) and helping to cultivate clear arguments and claims. If asked to review research that is not yet conducted (as in [the recent special issue of *Comparative Political Studies*](http://www.ipdutexas.org/cps-transparency-special-issue.html)), try to review with an eye toward improving the author's ability to make credible knowledge claims in the design being proposed.
 
 4. In your own work, engage in fully reproducible research where all data and code to reproduce published claims is fully transparent. Demand the same from authors.
 
 5. Preregistration your research when that preregistration process involves scientific peer-review and *guarantees* acceptance for eventual publication.

 6. Train your students to be better reviewers, to demand reproducibility, and to focus on the justifiability of knowledge claims and not the content of those claims.
 
Things academic societies and their journals can do:

 1. Publish the names of article reviewers and the content of those reviews alongside published articles. When something is peer reviewed, that is a badge that the research is considered to be making justified knowledge claims. A problem, however, relates to how different reviewers might evaluate an article. Publishing the names of the reviewers (even if those names were originally blind to the author), indicate exactly who evaluated the credibility of the claims being made. Publishing names also gives long-due credit to peer reviewers, so that those contributions can be weighed (if appropriate) in decisions about hiring and promotion. And societies should encourage departments to value the important public service of peer review. After publishing reviewers' names, publishing the actual reviews themselves is the obvious next step. Anonymity allows reviewers to use unprofessional language, author terse, unhelpful reviews, and potentially discriminate against research that they find objectionable for reasons other than scientific quality. Publishing the content of the reviews places accountability pressures that should enhance quality and protect against person vendettas during the review process.
 
 2. Shift to methods of pre-analysis and post-publication review processes. Peer review primarily helps to evaluate the quality of research designs and research methods. As such, significant additional value can be extracted from peer review if that process occurs before research is conducted, when there is still time to make changes. Such pre-reviewed papers should also be pre-accepted for publication by the reviewing organization, thereby eliminating result-dependent publication decisions. Similarly, because post-analysis peer review adds little value on top of that, articles should be open for post-publication peer review where other scientists can publicly comment on published articles to highlight weaknesses in design and analysis - comments that potentially invite post-publication revisions or further research.
 
 3. Eliminate the print journal as the venue for disseminating research, or dramatically relax constraints on the number of articles published by a journal. Instead, articles should be published in large-scale, open access repositories that allow for easy public, educational, and scholarly access. A major function of journals is to serve as recommendation engines: an article being published in a journal is a signal from the editor (usually a distinguished scholar in the field) that this article should be read and cited. We don't need journals to provide this kind of recommendation. Citations and altmetrics now provide easy measures by which to evaluate the eventual impact of research. Individual scholars, research groups, and academic societies can also produce recommendation lists for articles independent of those articles being published in a specific venue. Almost all research has informational value that can be useful in updating our beliefs about the world. There is no need in the present era to restrict the collective amount of research output, so any shift in publication processes should also be scalable to a much-higher level of research output.
 
