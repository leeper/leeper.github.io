---
layout: post
title: "Peer-Improved, Not Peer-Reviewed, Journals"
description: "Academic journals need to pivot from reviewing research to actively improving it."
tags:
 - academia
 - peer review
 - journals
 - publishing
---

An interesting aspect of contemporary science is that the fast-paced development of new research combined with alternative platforms for disseminating findings (e.g., blogs and social media) mean that research frequently gets exposure, feedback, criticism, praise, journalistic attention, citations, and even replication long before it passes through a traditional peer review process. As [I've written before](https://thepoliticalmethodologist.com/2015/12/21/the-multiple-routes-to-credibility/), this means that research is effectively being peer reviewed more quickly and more broadly than ever before. Rather than peer review coming primarily through the institutionalized form of anonymous vetting by anonymous reviewers, peer review now entails public, discursive, and cumulative engagement with research. Many have criticized this reviewing transition for being too fast, too open, too informal, and too critical. Those criticisms aside, fast, public peer review seems here to stay. My question today is therefore: what does this mean for traditional journals?

While we can continue to pretend that academic journals remain the definitive source of the scientific record, books, preprints, working papers, conference presentations, blog posts, and even ephemeral communication forms like gists, github repos, and tweets increasingly contain insights that become vital to collective knowledge. Journals were created to solve coordination problems around the sharing of scholarly insights; other platforms now provide equally viable means of research communication. Journals introduced peer review to vet unfamiliar and thus potentially untrustworthy authors; fast, public peer review provides the same without regard to authorship and with the perk of public accountability. Fast, public peer review hasn't replaced traditional journals and their traditional review processes but it presents a parallel and quite different means of checking the validity of research.

Journals therefore provide two functions - communication and vetting - that are rapidly being solved by other means that are quicker, supported by broader and more diverse research communities, and less expensive that the for-profit publishing models that have been built-up around prestigious academic journals. Amidst this competition, journals need to clarify their purpose in a radically faster, more public scientific world. They need to adjust their mission in these changing times and demonstrate value beyond communication and vetting. Journals need a new role in science and new processes and incentive structures to support that role.

What I have in mind is this: the peer reviewed journal is dead or at least dying. If not for incentives that tie publication in academic journals to promotion, journals would have died as soon as their communicative function was replaceable. Journals discourage citations to preprints and working papers, in part, because publishers know that a world where such forms of scientific reporting count equally to articles is a world where journals are expensive legacy platforms. Similarly, top-tier general journals typically discourage sharing preprints pre-publication as they desire to reiterate their premier position as venues for communicating research. These types of policies aim to prevent the loss of market share without any meaningful innovation.

An academic journal therefore needs to be something other than its minimal type: it needs to do more than communicate and vet. Let's reason a bit by analogy. Companies often die because they lose sight of what their product is. The now-classic example is Kodak. Kodak went bankrupt because they thought they were in the business of film. But digital camera technology disrupted the film camera world. By thinking they were selling film, Kodak missed the underlying reality that they were selling a means of sharing memories. How best do you share memories? Once upon a time it was silver-plate photographs taken by a professional photographer, then it was black-and-white photographs taken on a camera, then it was color photographs taken on a camera, but then it quickly became film and digital and photo-sharing websites and beyond. Kodak thought they sold films but actually they sold something else and other companies sold the public technologies to save and share memories better than the thing Kodak thought was their core product.

That might seem like an oblique analogy but it gets at the core of what I want to argue: journals do not sell communication and vetting, or at least not just that anymore. Rather, journals sell curation. Other people also sell curation. I sell curation: I tell people to read things. [John Holbein](https://twitter.com/johnholbein1) sells curation: he tweets new social science papers that people should read. Anyone can sell curation but journals have a reputation for doing it. People read journals not just because that's where research is or that's where research has been vetted. They read journals because there's a fairly widely held belief that *good* research is published there; that limited time spent consuming research should be spent there rather than elsewhere. Journals sell search filters, not peer review.

"Okay, Thomas, that's great but this TED Talk is getting a bit long," you're saying. You're right. Let's wrap this up by answering my initial question: what does this mean for journals? I think it means that journals need to pivot. Journals are structured entirely around the peer review process: accept arbitrary submissions, subject them to editorial and/or peer review, allow revision of some, and publish a tiny fraction of the total. But in a world where journals are recognized as being about filtering rather than reviewing, I think journals should rebrand themselves and restructure their internal operations. Here's how:

 1. Journals should see themselves as "peer-improved" rather than "peer-reviewed". Most research is heavily reviewed and, ultimately, individual scientists will disagree on how science should be practiced and where something is good enough. Journals should therefore focus on rapid desk rejection of papers they don't think are improvable. Rather than rejecting because something is too new, journals should judge research on its potential. Such a frame of reference naturally invites results-blind review, preregistration, replication, and other routes to research credibility.
 2. Journals should recognize and cultivate the communities of authors, reviewers, and readers they support. Peer-improved journals should first and foremost be useful. They should help improve the research performed by their contributors. They should help ensure that their community receives the research they need to do their work. Metrics of community well-being should replace metrics of citation counts and impact factors; these traditional metrics should follow naturally from healthy journal communities. Shifting metrics will help accommodate different forms and lenghts of contribution without publishers worrying about increasing the denominator by which citation counts are divided.
 3. "General" journals should be given much less recognition than they currently receive. If peer-improved journals primarily curate and filter, while providing inclusive scholarly communitities, then there's almost no research that belongs in a discipline-spanning outlet. That is a sign that the work is not especially relevant to the incremental work done by the research community surrounding an applied problem. Obviously, field-spanning broad journals have a purpose in a world of small communitities but academic incentives should be aligned to publishing the best research in the most appropriate places rather than pretending that generality is a synonym for quality.
 4. The work of peer-improved journals should be to improve science. Reviewers should be invested in the idea that their work is to make research outputs better, not put them or their authors down. Reviewers should be incentivized and recognized for these positive contributions. Peer-improvement is a task of encouragement, mentoring, and feedback, not a purely critical dismissal of authors' effort and capability.
 5. Even more radically, we may need to think about journals themselves as ephemeral. Biopolitics could have benefitted from a journal in the mid-2000s where scholars worked together to improve research on genetic bases of political orientations but with the literature moving on, such a journal feels less useful than it was a decade ago. Field experimentalists could have benefitted from a well-focused effort in the early 2000s before the method became diffuse enough that it was relevant to a larger number of communities. Polarization researchers could probably use a journal now to quickly develop questions, methods, and data, but in a decade's time it's hopefully the case that it wouldn't be particularly useful to continue. Journals should come and go as needed.

These are some of the key ways that I think disrupting academic journals could be useful by rethinking their purpose in a world where their superficial purposes are no longer unique. Ultimately, in a world of fast, public peer review, we don't need journals to communicate science and we don't need the current peer-review processes they provide to improve science. We can achieve both through other means, more quickly, more cheaply, more inclusively, and more effectively. What we do need are filters and journals are well-positioned to provide those filters, provided they are also willing to match curation with cultivation, review with investment, and criticism with constructive feedback. And we need universities to recognize the value to humanity of individual efforts to improve science, not just understand science as one-off articles landed in high-prestige outlets.
